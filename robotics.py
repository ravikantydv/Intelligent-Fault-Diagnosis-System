# -*- coding: utf-8 -*-
"""Robotics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Db5FtbA_ZDQqcn_2fv57EBkIrveQY9bW
"""

from google.colab import drive
import os
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

from google.colab import drive
drive.mount('/content/drive')

# Set the path to the train data
# lp1_path = "/content/lp1.data"
# lp2_path = "/content/lp2.data"
# lp3_path = "/content/lp3.data"
# lp4_path = "/content/lp4.data"
# lp5_path = "/content/lp5.data"

lp1_path = "/content/drive/MyDrive/Datasets/DataFile/lp1.data"
lp2_path = "/content/drive/MyDrive/Datasets/DataFile/lp2.data"
lp3_path = "/content/drive/MyDrive/Datasets/DataFile/lp3.data"
lp4_path = "/content/drive/MyDrive/Datasets/DataFile/lp4.data"
lp5_path = "/content/drive/MyDrive/Datasets/DataFile/lp5.data"

# Function to read and process a file

def process_file(file_path):
    try:
        with open(file_path, 'r') as file:
            print(f"Processing file: {file_path}")

            # Read the contents of the file
            data = file.read()

            # Split the data into blocks based on the category
            blocks = data.split('\n\n')

            # Process each block
            for block in blocks:
                # Split the block into lines
                lines = block.split('\n')

                # Extract and print the category
                if lines:
                    category = lines[0].strip()
                    print(f"Category: {category}")

                    # Print the numerical values in each line
                    for line in lines[1:]:
                        values = line.split('\t')
                        print(values)

    except FileNotFoundError:
        print(f"File not found: {file_path}")

    except Exception as e:
        print(f"An error occurred: {e}")

# List of file paths
file_paths = [lp1_path, lp2_path, lp3_path, lp4_path, lp5_path]

# Process each file
for file_path in file_paths:
    process_file(file_path)

# Function to read and process a file
def process_file(file_path, merged_data):
    try:
        with open(file_path, 'r') as file:
            print(f"Processing file: {file_path}")

            # Read the contents of the file
            data = file.read()

            # Split the data into blocks based on the category
            blocks = data.split('\n\n')

            # Process each block
            for block in blocks:
                # Split the block into lines
                lines = block.split('\n')

                # Extract and print the category
                if lines:
                    category = lines[0].strip()
                    print(f"Category: {category}")

                    # Append the numerical values to the merged data
                    for line in lines[1:]:
                        values = line.split('\t')
                        merged_data.append([category] + values)

    except FileNotFoundError:
        print(f"File not found: {file_path}")

    except Exception as e:
        print(f"An error occurred: {e}")

# List of file paths
file_paths = [lp1_path, lp2_path, lp3_path, lp4_path, lp5_path]

# Accumulate merged data
merged_data = []

# Process each file
for file_path in file_paths:
    process_file(file_path, merged_data)

# Write merged data to a new file
output_file_path = "/content/merged_data.txt"
with open(output_file_path, 'w') as output_file:
    for row in merged_data:
        output_file.write('\t'.join(map(str, row)) + '\n')

print(f"Merged data written to: {output_file_path}")

# Specify the path to the merged data file
merged_data_path = "/content/merged_data.txt"

import pandas as pd

try:
    with open(merged_data_path, 'r', encoding='utf-8') as merged_file:
        # Read the contents of the merged file
        merged_data_content = merged_file.read()

        # Split the content into lines
        merged_lines = merged_data_content.split('\n')

        # Initialize lists to store features and labels
        all_features = []
        all_labels = []

        # Process data in chunks (15 columns for features + 1 column for label)
        for i in range(0, len(merged_lines), 16):
            # Extract features
            features_line = merged_lines[i:i+15]
            # Check if all lines have the expected number of values
            if all(len(line.split('\t')) == 8 for line in features_line):
                features = [line.split('\t') for line in features_line]
                all_features.append(features)
            else:
                print(f"Skipping invalid features at line {i+1}")
                continue

            # Extract label
            label_line = merged_lines[i+15]
            label_values = label_line.split('\t')
            all_labels.append(label_values)

        # Create a DataFrame for features
        features_df = pd.DataFrame(all_features, columns=[f'feature_{i}' for i in range(1, 16)])

        # Create a DataFrame for labels
        labels_df = pd.DataFrame(all_labels, columns=['label_1', 'label_2'])

        # Combine features and labels DataFrames
        combined_df = pd.concat([features_df, labels_df], axis=1)

        # Display the combined DataFrame
        print(combined_df)

except FileNotFoundError:
    print(f"Merged data file not found: {merged_data_path}")

except Exception as e:
    print(f"An error occurred while reading the merged data file: {e}")

# Specify the path to the merged data file
merged_data_path = "/content/merged_data.txt"

try:
    with open(merged_data_path, 'r') as merged_file:
        # Read the contents of the merged file
        merged_data_content = merged_file.read()

        # Split the content into lines
        merged_lines = merged_data_content.split('\n')

        # Print the first 10 lines or adjust the range as needed
        for line in merged_lines[:1000]:
            # Split the line into values
            values = line.split('\t')

            # Print or process the values as needed
            print(values)

except FileNotFoundError:
    print(f"Merged data file not found: {merged_data_path}")

except Exception as e:
    print(f"An error occurred while reading the merged data file: {e}")

combined_df.head()

combined_df.isnull().sum()

# Drop the first two values from each feature column
for i in range(1, 16):
    combined_df[f'feature_{i}'] = combined_df[f'feature_{i}'].apply(lambda x: x[2:] if isinstance(x, list) and len(x) > 2 else np.nan)

combined_df.head()

#flattening the values
for i in range(1, 16):
    combined_df[f'feature_{i}'] = combined_df[f'feature_{i}'].apply(lambda x: [x] if isinstance(x, int) else x)

# Concatenate the flattened lists into separate columns
flattened_features = pd.concat([combined_df[f'feature_{i}'].apply(pd.Series) for i in range(1, 16)], axis=1)

# Update the original DataFrame with the flattened features
combined_df = pd.concat([combined_df.drop([f'feature_{i}' for i in range(1, 16)], axis=1), flattened_features], axis=1)

unique_labels_1 = combined_df['label_1'].unique()
print(unique_labels_1)

#Label_1 is of no use we can drop it
combined_df = combined_df.drop(columns=['label_1'])

# Check the data types of each column in the DataFrame
data_types = combined_df.dtypes

# Display the data types
print(data_types)

unique_labels_2 = combined_df['label_2'].unique()
print(unique_labels_2)

# Exclude 'label_2' from conversion
columns_to_convert = combined_df.columns.difference(['label_2'])

# Convert selected columns to float, coerce errors to NaN
combined_df[columns_to_convert] = combined_df[columns_to_convert].apply(pd.to_numeric, errors='coerce')

# Check the data types again
print(combined_df.dtypes)

combined_df.head()

data = combined_df.copy()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Encode the target variable 'label_2' in the original DataFrame
label_encoder = LabelEncoder()
data['label_2'] = label_encoder.fit_transform(data['label_2'])

X = data.drop('label_2', axis=1)
y = data['label_2']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_train)

# Visualize explained variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)
import matplotlib.pyplot as plt

# Plot explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')

plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

# Initialize the classifier with suggested hyperparameters
ann_classifier = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # Example architecture, adjust as needed
    max_iter=500,
    random_state=42,
    alpha=0.0001,  # L2 regularization term
    learning_rate='adaptive',  # Adjust the learning rate dynamically
    early_stopping=True,  # Enable early stopping
    validation_fraction=0.1,  # Fraction of training data to be used as validation set
)

# Train the model
ann_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = ann_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}\n')
print('Classification Report:\n', classification_report(y_test, y_pred))

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'hidden_layer_sizes': [(100,), (50, 25), (50, 50)],
    'alpha': [0.0001, 0.001, 0.01],
}

# Initialize the classifier
ann_classifier = MLPClassifier(max_iter=500, random_state=42)

from sklearn.model_selection import GridSearchCV

# Perform GridSearchCV
grid_search = GridSearchCV(ann_classifier, param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters:", grid_search.best_params_)

# Get the best model
best_ann_classifier = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_ann_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}\n')
print('Classification Report:\n', classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Training the model and capturing the loss history
history = best_ann_classifier.fit(X_train, y_train).loss_curve_

# Plot the loss curve
plt.plot(history)
plt.title('Loss Curve During Training')
plt.xlabel('Number of Iterations')
plt.ylabel('Loss')
plt.show()

"""Uncomment the below code for more optimised results and will take around 40-45 minutes."""

# # Parameter grid for hyperparameter tuning
# param_grid = {
#     'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],
#     'max_iter': [500, 1000, 1500],
#     'alpha': [0.0001, 0.001, 0.01],
# }

# Create MLPClassifier
# mlp = MLPClassifier(random_state=42)

# Create GridSearchCV
# grid_search = GridSearchCV(mlp, param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the model
# grid_search.fit(X_train_smote, y_train_smote)

# Get the best parameters
# best_params = grid_search.best_params_
# print(f"Best Hyperparameters: {best_params}")

# Make predictions on the test set
# y_pred = grid_search.predict(X_test)

# # Print accuracy and classification report
# accuracy = accuracy_score(y_test, y_pred)
# print(f'Accuracy: {accuracy:.4f}\n')
# print('Classification Report:\n', classification_report(y_test, y_pred))

# Plot the results
def plot_results2(x_train, y_train, y_pred):
    plt.plot(x_train, label="Training Data")
    #plt.plot(np.arange(len(x_train), len(x_train) + len(y_train)), y_train, label="True Values", color="blue")
    #plt.plot(np.arange(len(x_train), len(x_train) + len(y_pred)), y_pred, label="GP Prediction", color="red")

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from sklearn.metrics import accuracy_score, classification_report

# Create SVC
svm_classifier = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)

# Train the SVM model
svm_classifier.fit(X_train, y_train)

# Make predictions on the test set
svm_predictions = svm_classifier.predict(X_test)

# Print accuracy and classification report
svm_accuracy = accuracy_score(y_test, svm_predictions)
print(f'SVM Accuracy: {svm_accuracy:.4f}\n')
print('SVM Classification Report:\n', classification_report(y_test, svm_predictions))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier

# Create and train the model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
dt_predictions = dt_model.predict(X_test)

# Evaluate the model
dt_accuracy = accuracy_score(y_test, dt_predictions)
print(f'Decision Tree Accuracy: {dt_accuracy:.4f}')
print('Classification Report:\n', classification_report(y_test, dt_predictions))

# Create and train the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
rf_predictions = rf_model.predict(X_test)

# Evaluate the model
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'Random Forest Accuracy: {rf_accuracy:.4f}')
print('Classification Report:\n', classification_report(y_test, rf_predictions))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Create and train the model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Make predictions
knn_predictions = knn_model.predict(X_test)

# Evaluate the model
knn_accuracy = accuracy_score(y_test, knn_predictions)
print(f'k-NN Accuracy: {knn_accuracy:.4f}')
print('Classification Report:\n', classification_report(y_test, knn_predictions))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Create and train the model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# Make predictions
lr_predictions = lr_model.predict(X_test)

# Evaluate the model
lr_accuracy = accuracy_score(y_test, lr_predictions)
print(f'Logistic Regression Accuracy: {lr_accuracy:.4f}')
print('Classification Report:\n', classification_report(y_test, lr_predictions))